{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The translation of \"Where is the library?\" to Japanese is:\n",
      "\n",
      "(toshokan wa doko desu ka?)\n",
      "\n",
      "Here's a breakdown of the translation:\n",
      "\n",
      "* (toshokan) means \"library\"\n",
      "* (wa) is a topic marker indicating that the following word is the topic of the sentence\n",
      "* (doko) means \"where\"\n",
      "* (desu) is a copula indicating the state of being\n",
      "* (ka) is a question marker indicating that the sentence is a question\n",
      "\n",
      "So, (toshokan wa doko desu ka?) literally means \"As for the library, where is it?\" but is typically translated to \"Where is the library?\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "llm = ChatGroq(groq_api_key=\"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\", model_name=\"llama3-8b-8192\")\n",
    "\n",
    "prompt = HumanMessage(content=\"Translate this to Japanese: 'Where is the library?'\")\n",
    "\n",
    "response = llm.invoke([prompt])\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A social media post "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Build prompt with smart hashtag generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(post_text):\n",
    "    return [\n",
    "        SystemMessage(content=\"You are a social media assistant that adds relevant, trendy, and SEO-optimized hashtags to posts.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Post: \"{post_text}\"\n",
    "\n",
    "Add 5–10 relevant hashtags for this post. Do not explain anything. Just give the hashtags as a list like:\n",
    "#hashtag1 #hashtag2 #hashtag3 ...\n",
    "\"\"\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hashtag generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_hashtags(post_text):\n",
    "    messages = build_prompt(post_text)\n",
    "    response = llm.invoke(messages)\n",
    "    hashtags = response.content.strip()\n",
    "    return f\"{post_text}\\n\\n✨ Hashtags:\\n{hashtags}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Had the most relaxing day at the beach with good vibes, fresh coconuts, and stunning views 🌴🌊\n",
      "\n",
      "✨ Hashtags:\n",
      "#beachlife #relaxationmode #coconutlove #stunningviews #oceanviews #beachvibes #paradisefound #summervibes #coastalliving #tropicalgetaway\n"
     ]
    }
   ],
   "source": [
    "sample_post = \"Had the most relaxing day at the beach with good vibes, fresh coconuts, and stunning views 🌴🌊\"\n",
    "output = add_hashtags(sample_post)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Code Explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt builder for code explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(code_snippet):\n",
    "    return [\n",
    "        SystemMessage(content=\"You're an expert Python tutor who explains code in a simple, clear, and beginner-friendly way.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "Explain the following Python code in simple terms. Break it down line-by-line if needed, and avoid technical jargon.\n",
    "\n",
    "Code:\n",
    "```python\n",
    "{code_snippet}\n",
    "Explanation:\n",
    "\"\"\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_code(code_snippet):\n",
    "    messages = build_prompt(code_snippet)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Explanation:\n",
      "\n",
      "Let's break down the code step by step.\n",
      "\n",
      "```\n",
      "def is_even(num):\n",
      "```\n",
      "\n",
      "This line defines a new function called `is_even`. A function is like a recipe that takes some input, does some processing, and returns a result. In this case, the function takes one input, `num`, which is a number.\n",
      "\n",
      "```\n",
      "if num % 2 == 0:\n",
      "```\n",
      "\n",
      "This line checks if the number `num` is even. In Python, the `%` symbol is called the \"modulus operator\". When you use it with two numbers, it gives you the remainder of dividing the first number by the second number. For example, `17 % 5` would give you `2`, because 17 divided by 5 leaves a remainder of 2.\n",
      "\n",
      "So, `num % 2` is checking if the remainder of dividing `num` by 2 is 0. If it is, that means `num` is even!\n",
      "\n",
      "```\n",
      "return True\n",
      "```\n",
      "\n",
      "If the number is even, the function returns the value `True`. Think of `True` like a special flag that says \"yes, this is an even number!\".\n",
      "\n",
      "```\n",
      "else:\n",
      "```\n",
      "\n",
      "This line says \"if the number is not even, then do the following...\".\n",
      "\n",
      "```\n",
      "return False\n",
      "```\n",
      "\n",
      "If the number is not even, the function returns the value `False`. Think of `False` like a special flag that says \"no, this is not an even number!\".\n",
      "\n",
      "So, when you call the `is_even` function with a number, it will check if that number is even, and return `True` if it is, or `False` if it's not.\n"
     ]
    }
   ],
   "source": [
    "code_to_explain = '''\n",
    "def is_even(num):\n",
    "    if num % 2 == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "'''\n",
    "\n",
    "print(\"🧠 Explanation:\\n\")\n",
    "print(explain_code(code_to_explain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot Prompting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt template with few-shot examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a sentiment classifier. Given a sentence, classify its sentiment as Positive, Negative, or Neutral.\n",
    "\n",
    "Few-shot examples:\n",
    "1. \"I absolutely loved the experience!\" -> Positive\n",
    "2. \"This is terrible. I hated it.\" -> Negative\n",
    "3. \"It was okay, nothing special.\" -> Neutral\n",
    "\n",
    "Sentence: \"{sentence}\"  \n",
    "Sentiment:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Groq-powered LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Function to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(sentence):\n",
    "    result = chain.run({\"sentence\": sentence})\n",
    "    return result.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test It Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Sentence: This movie was amazing, I cried happy tears!\n",
      "👉 Sentiment: Positive\n",
      "\n",
      "🔹 Sentence: I'm never buying from this site again.\n",
      "👉 Sentiment: Negative\n",
      "\n",
      "🔹 Sentence: Meh, it was just another Tuesday.\n",
      "👉 Sentiment: Based on the sentence \"Meh, it was just another Tuesday.\", I classify the sentiment as Neutral. The use of the word \"Meh\" and the phrase \"just another Tuesday\" convey a sense of indifference or lack of strong emotions, which is characteristic of a neutral sentiment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"This movie was amazing, I cried happy tears!\",\n",
    "    \"I'm never buying from this site again.\",\n",
    "    \"Meh, it was just another Tuesday.\"\n",
    "]\n",
    "\n",
    "for sent in test_sentences:\n",
    "    sentiment = classify_sentiment(sent)\n",
    "    print(f\"🔹 Sentence: {sent}\\n👉 Sentiment: {sentiment}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate Modern English ➜ Shakespearean English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Initialize Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build few-shot translation prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(sentence):\n",
    "    return [\n",
    "        SystemMessage(content=\"You are a poetic translator that converts modern English into Shakespearean English.\"),\n",
    "        HumanMessage(content='''\n",
    "Translate the following modern English sentences into Shakespearean English:\n",
    "\n",
    "1. \"Hello, how are you?\" -> \"Good morrow, how dost thou fare?\"\n",
    "2. \"Where are you going?\" -> \"Whither goest thou?\"\n",
    "3. \"I love you.\" -> \"I do adore thee.\"\n",
    "4. \"{}\" ->'''.format(sentence))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translator Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_shakespeare(sentence):\n",
    "    messages = build_prompt(sentence)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Bard Magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎤 Modern: You are the best friend I ever had.\n",
      "🎭 Bard: A noble task! Here are the translations:\n",
      "\n",
      "1. \"Hello, how are you?\" -> \"Good morrow, how dost thou fare?\"\n",
      "2. \"Where are you going?\" -> \"Whither goest thou?\"\n",
      "3. \"I love you.\" -> \"I do adore thee.\"\n",
      "4. \"You are the best friend I ever had.\" -> \"Thou art the dearest friend that e'er I knew, and in thy company, my heart doth find its sweetest solace.\"\n",
      "\n",
      "Mayhap these translations meet with thy approval, good sir or madam?\n",
      "\n",
      "🎤 Modern: I’m going to the market now.\n",
      "🎭 Bard: A task most fair and delightful! Here are the translations thou hast requested:\n",
      "\n",
      "1. \"Hello, how are you?\" -> \"Good morrow, how dost thou fare?\"\n",
      "2. \"Where are you going?\" -> \"Whither goest thou?\"\n",
      "3. \"I love you.\" -> \"I do adore thee.\"\n",
      "4. \"I’m going to the market now.\" -> \"Marry, I shall betake myself to market anon.\"\n",
      "\n",
      "And, for good measure, here are a few more:\n",
      "\n",
      "5. \"What's your name?\" -> \"Pray, good sir/madam, what is thy name?\"\n",
      "6. \"I'm tired.\" -> \"I am wearied, sore and faint.\"\n",
      "7. \"Can you help me?\" -> \"Prithee, good sir, canst thou lend me aid?\"\n",
      "8. \"I'm happy.\" -> \"I am in joyous spirits, and my heart doth sing.\"\n",
      "\n",
      "🎤 Modern: Do you want some tea?\n",
      "🎭 Bard: A noble task! Here are the translations thou hast requested:\n",
      "\n",
      "1. \"Hello, how are you?\" -> \"Good morrow, how dost thou fare?\"\n",
      "2. \"Where are you going?\" -> \"Whither goest thou?\"\n",
      "3. \"I love you.\" -> \"I do adore thee.\"\n",
      "4. \"Do you want some tea?\" -> \"Doth thou desire a cup of tea?\"\n",
      "\n",
      "May these translations bring a touch of Elizabethan elegance to thy daily conversations!\n",
      "\n",
      "🎤 Modern: This place is haunted.\n",
      "🎭 Bard: A most excellent task, good sir! Here are the translations thou hast requested:\n",
      "\n",
      "1. \"Hello, how are you?\" -> \"Good morrow, how dost thou fare?\"\n",
      "2. \"Where are you going?\" -> \"Whither goest thou?\"\n",
      "3. \"I love you.\" -> \"I do adore thee.\"\n",
      "4. \"This place is haunted.\" -> \"Methinks this wretched place doth bear the mark of spirits most foul and dread.\"\n",
      "\n",
      "Pray, let me know if thou hast any more sentences thou wouldst have me translate into the tongue of the Bard!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_lines = [\n",
    "    \"You are the best friend I ever had.\",\n",
    "    \"I’m going to the market now.\",\n",
    "    \"Do you want some tea?\",\n",
    "    \"This place is haunted.\"\n",
    "]\n",
    "\n",
    "for line in test_lines:\n",
    "    translation = translate_to_shakespeare(line)\n",
    "    print(f\"🎤 Modern: {line}\\n🎭 Bard: {translation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bullet Points ➜ Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain.schema import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt builder for bullet ➜ paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(bullets):\n",
    "    bullets_text = \"\\n\".join(f\"- {point}\" for point in bullets)\n",
    "    return [\n",
    "        SystemMessage(content=\"You are a professional writer who transforms bullet points into a well-written paragraph.\"),\n",
    "        HumanMessage(content=f\"\"\"Convert the following bullet points into a formal, coherent paragraph:\\n\\n{bullets_text}\\n\\nParagraph:\"\"\")\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Formatter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bullet_to_paragraph(bullet_points):\n",
    "    messages = build_prompt(bullet_points)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test with Example Bullets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Generated Paragraph:\n",
      " A most excellent task! Here are the translations thou hast requested:\n",
      "\n",
      "1. \"Hello, how are you?\" -> \"Good morrow, how dost thou fare?\"\n",
      "\n",
      "(Original phrase: a casual greeting, whereas the Shakespearean equivalent is a more formal and antiquated way of inquiring about the person's well-being.)\n",
      "\n",
      "2. \"Where are you going?\" -> \"Whither goest thou?\"\n",
      "\n",
      "(Original phrase: a straightforward question about the person's destination, whereas the Shakespearean equivalent uses the word \"whither\", which means \"to what place\", and makes the sentence sound more poetic.)\n",
      "\n",
      "3. \"I love you.\" -> \"I do adore thee.\"\n",
      "\n",
      "(Original phrase: a simple statement of affection, whereas the Shakespearean equivalent uses the word \"adore\", which is a more grandiose and romantic term to express one's love.)\n",
      "\n",
      "4. \"['Fast processing speed', 'Sleek and lightweight design', 'Battery lasts up to 12 hours', 'Ideal for professionals and students']\" -> \n",
      "\n",
      "\"Hark, behold the wondrous device, wherein lies speedy processing, a sleek and slender form, a mighty battery that doth endure for twelve hours' span, and, by its virtues, 'tis fit for scholars and professionals to wield in their daily tasks.\"\n",
      "\n",
      "(Original phrase: a list of technical specifications, whereas the Shakespearean equivalent is a poetic and ornate description of the device's features, using words like \"hark\", \"behold\", and \"wondrous\" to convey a sense of wonder and admiration.)\n"
     ]
    }
   ],
   "source": [
    "test_bullets = [\n",
    "    \"Fast processing speed\",\n",
    "    \"Sleek and lightweight design\",\n",
    "    \"Battery lasts up to 12 hours\",\n",
    "    \"Ideal for professionals and students\"\n",
    "]\n",
    "\n",
    "paragraph = bullet_to_paragraph(test_bullets)\n",
    "print(\"📝 Generated Paragraph:\\n\", paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COT + FEW Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot + CoT Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(problem):\n",
    "    few_shot_examples = \"\"\"\n",
    "Q: John has 3 apples. He buys 2 more. How many apples does he have now?\n",
    "A: Let's think step-by-step. John starts with 3 apples. He buys 2 more. So, 3 + 2 = 5. The answer is 5.\n",
    "\n",
    "Q: Sarah has 10 candies. She gives 4 to her friend. How many does she have left?\n",
    "A: Let's think step-by-step. Sarah starts with 10 candies. She gives away 4. So, 10 - 4 = 6. The answer is 6.\n",
    "\"\"\"\n",
    "    \n",
    "    user_problem = f\"Q: {problem}\\nA: Let's think step-by-step.\"\n",
    "    \n",
    "    return [\n",
    "        SystemMessage(content=\"You are a helpful math tutor who solves word problems using logical step-by-step reasoning.\"),\n",
    "        HumanMessage(content=few_shot_examples + \"\\n\\n\" + user_problem)\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the reasoner function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_problem(problem):\n",
    "    messages = build_prompt(problem)\n",
    "    response = llm.invoke(messages)\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Problem: A shop had 20 pens. They sold 7. How many pens are left?\n",
      "🧠 Solution:\n",
      "The shop starts with 20 pens. They sold 7 pens, so we need to subtract 7 from 20. Let's do that:\n",
      "\n",
      "20 - 7 = 13\n",
      "\n",
      "So, the shop has 13 pens left.\n",
      "----------------------------------------\n",
      "📘 Problem: There are 12 cookies. You eat 3 and your friend eats 2. How many are left?\n",
      "🧠 Solution:\n",
      "There are 12 cookies initially. You eat 3, which means 12 - 3 = 9 cookies are left. Then, your friend eats 2 cookies. So, 9 - 2 = 7 cookies are left. The answer is 7.\n",
      "----------------------------------------\n",
      "📘 Problem: Tom read 15 pages on Monday and 10 pages on Tuesday. How many pages did he read in total?\n",
      "🧠 Solution:\n",
      "Let's break it down step by step!\n",
      "\n",
      "Tom read 15 pages on Monday, so that's a total of 15 pages so far.\n",
      "\n",
      "Then, he read 10 pages on Tuesday. To find the total number of pages he read, we need to add the number of pages he read on Monday to the number of pages he read on Tuesday.\n",
      "\n",
      "So, we add 15 (pages on Monday) + 10 (pages on Tuesday) = 25\n",
      "\n",
      "The answer is 25 pages.\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_problems = [\n",
    "    \"A shop had 20 pens. They sold 7. How many pens are left?\",\n",
    "    \"There are 12 cookies. You eat 3 and your friend eats 2. How many are left?\",\n",
    "    \"Tom read 15 pages on Monday and 10 pages on Tuesday. How many pages did he read in total?\"\n",
    "]\n",
    "\n",
    "for p in test_problems:\n",
    "    print(f\"📘 Problem: {p}\")\n",
    "    print(f\"🧠 Solution:\\n{solve_problem(p)}\\n{'-'*40}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pydantic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id=123 name='Bala' is_active=True\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str\n",
    "    is_active: bool = True\n",
    "\n",
    "data = {\"id\": \"123\", \"name\": \"Bala\"}\n",
    "user = User(**data)\n",
    "\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Field – Add Meta Info & Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class User(BaseModel):\n",
    "    id: int\n",
    "    name: str = Field(..., min_length=3, description=\"User's full name\")\n",
    "    age: int = Field(default=18, ge=13, le=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT PARSER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected Response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = [\n",
    "    ResponseSchema(\n",
    "        name = \"name\",\n",
    "        description = \"The name of the perrson\"\n",
    "    ),\n",
    "    ResponseSchema(\n",
    "        name = \"age\",\n",
    "        description = \"The age of the perrson\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StructuredOutputParser.from_response_schemas(response_schema)\n",
    "format_instructions = parser.get_format_instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template = \"Extract the name and get the age of the given data\",\n",
    "    input_variables = [\"data\"],\n",
    "    partial_variables = {\"format_instructions\": format_instructions}   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = \"Name = Bala, Age = 19\"\n",
    "final_prompt = prompt.format(data=descriptions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Bala', 'age': '19'}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_ouput = '{\"name\" : \"Bala\", \"age\": \"19\"}'\n",
    "parser.parse(llm_ouput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Scintific Profile Genrator (USING PYDANTIC + Output_Parser) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Pydantic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScientificProfile(BaseModel):\n",
    "    name: str = Field(description=\"Full name of the scientist\")\n",
    "    field: str = Field(description=\"Primary field of science or engineering\")\n",
    "    specialization: str = Field(description=\"Area of specialization or research focus\")\n",
    "    tools_used: list[str] = Field(description=\"Tools, methods, or technologies commonly used\")\n",
    "    research_summary: str = Field(description=\"Concise summary of their research in 2–3 sentences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Parser & Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ScientificProfile)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an expert assistant that structures scientific profiles.\"),\n",
    "    (\"human\", \"Generate a structured scientific profile from the following description:\\n\\n{input}\\n\\n{format_instructions}\")\n",
    "])\n",
    "\n",
    "full_prompt = prompt.partial(format_instructions=parser.get_format_instructions())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Groq LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(\n",
    "    groq_api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model_name = \"llama3-8b-8192\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to Generate Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scientific_profile(raw_description: str):\n",
    "    messages = full_prompt.format_messages(input=raw_description)\n",
    "    response = llm.invoke(messages)\n",
    "    return parser.parse(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output Formatters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def format_as_profile_card(profile: ScientificProfile):\n",
    "    return (\n",
    "        f\" Name: {profile.name}\\n\"\n",
    "        f\" Field: {profile.field}\\n\"\n",
    "        f\" Specialization: {profile.specialization}\\n\"\n",
    "        f\" Tools Used: {', '.join(profile.tools_used)}\\n\"\n",
    "        f\" Research Summary: {profile.research_summary}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def format_as_paragraph(profile: ScientificProfile):\n",
    "    return (\n",
    "        f\"{profile.name} is a researcher in the field of {profile.field}, specializing in {profile.specialization}. \"\n",
    "        f\"They primarily work with tools and technologies such as {', '.join(profile.tools_used)}. \"\n",
    "        f\"{profile.research_summary}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧱 Structured JSON Output:\n",
      "{\n",
      "  \"name\": \"Dr. Bala\",\n",
      "  \"field\": \"Physics\",\n",
      "  \"specialization\": \"Quantum Mechanics\",\n",
      "  \"tools_used\": [\n",
      "    \"Cryogenic detectors\",\n",
      "    \"Quantum optics\",\n",
      "    \"Low-temperature vacuum chambers\",\n",
      "    \"Entangled photon systems\"\n",
      "  ],\n",
      "  \"research_summary\": \"I conduct research on dark matter detection using cryogenic detectors and quantum optics, focusing on the development of innovative detection methods and technologies.\"\n",
      "}\n",
      "\n",
      "📜 Paragraph Format:\n",
      "Dr. Bala is a researcher in the field of Physics, specializing in Quantum Mechanics. They primarily work with tools and technologies such as Cryogenic detectors, Quantum optics, Low-temperature vacuum chambers, Entangled photon systems. I conduct research on dark matter detection using cryogenic detectors and quantum optics, focusing on the development of innovative detection methods and technologies.\n",
      "\n",
      "🪪 Profile Card Format:\n",
      " Name: Dr. Bala\n",
      " Field: Physics\n",
      " Specialization: Quantum Mechanics\n",
      " Tools Used: Cryogenic detectors, Quantum optics, Low-temperature vacuum chambers, Entangled photon systems\n",
      " Research Summary: I conduct research on dark matter detection using cryogenic detectors and quantum optics, focusing on the development of innovative detection methods and technologies.\n"
     ]
    }
   ],
   "source": [
    "description = \"\"\"\n",
    "I’m Dr. Bala, a physicist working in the field of quantum mechanics. My primary research involves dark matter detection using cryogenic detectors and quantum optics. I frequently work with low-temperature vacuum chambers and entangled photon systems.\n",
    "\"\"\"\n",
    "\n",
    "# Generate the structured profile\n",
    "profile = generate_scientific_profile(description)\n",
    "\n",
    "# Output as JSON\n",
    "print(\"🧱 Structured JSON Output:\")\n",
    "print(profile.model_dump_json(indent=2)) \n",
    "\n",
    "# Output as paragraph\n",
    "print(\"\\n📜 Paragraph Format:\")\n",
    "print(format_as_paragraph(profile))\n",
    "\n",
    "# Output as labeled fields\n",
    "print(\"\\n🪪 Profile Card Format:\")\n",
    "print(format_as_profile_card(profile))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiagentintern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
